{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Subject: Pyspark\n",
        "# Author: Saran G\n",
        "# File Created On: 01 December 2023\n",
        "# Date of Submission: 27 January 2023"
      ],
      "metadata": {
        "id": "1ZwKAmMe1BN0"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Necessary Files"
      ],
      "metadata": {
        "id": "tFb_WVlUvQmt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtx4X27UvVOv",
        "outputId": "90f7eb0c-d19c-4e2c-c6b3-6c65d47f763b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845512 sha256=d4eb6e6e9c062eb76350df9e1cf93c7cfa9da91ef752c5d568c1ae85e89f986e\n",
            "  Stored in directory: /root/.cache/pip/wheels/43/dc/11/ec201cd671da62fa9c5cc77078235e40722170ceba231d7598\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark = SparkSession.builder.appName('Saran Assignment').getOrCreate()\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "E6fC46Z2vUNd",
        "outputId": "31d9ca5a-01ab-417f-c634-c698315aee55"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7fdf6125e490>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://f1c4e3637db9:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.3.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Saran Assignment</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "\n",
        "def path(link, filename):\n",
        "  url = f\"https://drive.google.com/uc?id={link}\"\n",
        "  gdown.download(url, filename, quiet=False)\n",
        "\n",
        "\n",
        "path(\"1fzb-Nhdb_dmn3oMQrkzmnKZMFAX00IZW\", \"2014-summary.csv\")\n",
        "path(\"1lUfnGU_v1ZUw1dRrovASH9M5BgYeAyxQ\",\"2015-summary.csv\")\n",
        "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"2015-summary.csv\")\n",
        "df2 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"2014-summary.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCA3ouFnvdHA",
        "outputId": "05070d7a-c2b6-4859-b23f-e871d8072eae"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1fzb-Nhdb_dmn3oMQrkzmnKZMFAX00IZW\n",
            "To: /content/2014-summary.csv\n",
            "100%|██████████| 6.73k/6.73k [00:00<00:00, 9.84MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1lUfnGU_v1ZUw1dRrovASH9M5BgYeAyxQ\n",
            "To: /content/2015-summary.csv\n",
            "100%|██████████| 7.08k/7.08k [00:00<00:00, 5.96MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Day 1 (chapters 1 & 2 from Spark:The definitive guide)"
      ],
      "metadata": {
        "id": "j9mThWFcvgeD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRJ1JTrchwr3"
      },
      "source": [
        "**1. What is bigdata?**\n",
        "\n",
        "Big data is a large amount of data that is generated and collected by organizations, which can be used to gain valuable insights and drive business growth.\n",
        "\n",
        "**For example,** a retail company might have big data that includes customer purchase history, website traffic data, and social media interactions. A healthcare provider might have big data that includes patient medical records, insurance claims, and sensor data from medical devices.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_F_2DEmiD6x"
      },
      "source": [
        "**2. Why Spark?**\n",
        "\n",
        "Apache Spark is a tool that helps with processing big data quickly, easily and efficiently. It can handle large amounts of data, work with many different types of data and is easy to use. It can be integrated with other tools and has a lot of support from the community, which helps it improve and grow.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfawVQ6iibzK"
      },
      "source": [
        "**3. What is Spark?**\n",
        "\n",
        "Imagine you are a retailer and you have millions of customer purchase records, website traffic data, and social media interactions stored in your data warehouse. You would like to analyze this data to gain insights about your customers, such as their purchasing habits, what products they like, and what promotions they respond to.\n",
        "\n",
        "Normally, analyzing such a large amount of data would be a time-consuming and difficult task. But, by using Spark, you can easily process and analyze this data in a matter of minutes. Spark allows you to write simple code to filter, aggregate, and analyze the data in parallel across many machines. This means that you can quickly gain insights from your data and make better-informed business decisions.\n",
        "\n",
        " Spark is a powerful tool that allows you to easily process and analyze big data, whether it's stored in a data warehouse, a NoSQL database or a distributed file system like HDFS. With Spark you can easily filter, aggregate, and analyze big data sets, perform advanced analytics, and gain insights from your data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkGtIi1di0PL"
      },
      "source": [
        "**4. Internals of spark?**\n",
        "\n",
        "Apache Spark is a distributed computing system that is composed of a driver program and worker nodes. RDDs are the fundamental data structure in Spark and are used to parallelize data processing. Data shuffling, task scheduler, and caching are some important mechanisms that Spark uses to improve the performance and usability of big data processing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NEQ4-kMjd10"
      },
      "source": [
        "**5. Highlevel API of spark? Sparksession, Dataframe, Partitions, Transformation, Actions, Lazy Evaluation**\n",
        "\n",
        "Highlevel API of spark include DataFrame API and Dataset API, which provides a convenient and easy-to-use interface for working with big data in Spark, these APIs are built on top of RDD API and provide more expressive way of working with structured data and additional features such as support for type inference, compile-time type checking, and improved performance.\n",
        "\n",
        "\n",
        "SparkSession is the entry point to Spark programming, DataFrames are distributed collection of data, Partitions are smaller chunks of data, Transformations are operations on DataFrames, Actions are operations that return value or produce a side effect and Lazy Evaluation is a technique where transformations are not executed until an action is called."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Day 2"
      ],
      "metadata": {
        "id": "nQiULNkRv5Xa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8IcfKvhRaoz"
      },
      "source": [
        "## SCHEMA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXcTtWKCRUKt",
        "outputId": "2a10a193-f1cd-41d1-b3e8-19f92045bb1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
            " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
            " |-- count: long (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
        "myManualSchema = StructType([\n",
        "StructField(\"DEST_COUNTRY_NAME\", StringType(), True),\n",
        "StructField(\"ORIGIN_COUNTRY_NAME\", StringType(), True),\n",
        "StructField(\"count\", LongType(), False)\n",
        "])\n",
        "df = spark.read.format(\"csv\")\\\n",
        ".schema(myManualSchema)\\\n",
        ".load(\"2015-summary.csv\")\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7I_L2gjSKHi"
      },
      "source": [
        "## COLUMNS AND EXPRESSIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9qp7SUHcR_42",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb199cc0-3b0d-4807-d34d-18656bdd1163"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+\n",
            "|count|\n",
            "+-----+\n",
            "| null|\n",
            "|   15|\n",
            "|    1|\n",
            "|  344|\n",
            "|   15|\n",
            "|   62|\n",
            "|    1|\n",
            "|   62|\n",
            "|  588|\n",
            "|   40|\n",
            "|    1|\n",
            "|  325|\n",
            "|   39|\n",
            "|   64|\n",
            "|    1|\n",
            "|   41|\n",
            "|   30|\n",
            "|    6|\n",
            "|    4|\n",
            "|  230|\n",
            "+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col, column\n",
        "df.select(col(\"count\")).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOUc19aGTg5l",
        "outputId": "2213a740-451a-44a7-91a6-85a3efc2cf93"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Column<'((((count + 5) * 200) - 6) < count)'>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "from pyspark.sql.functions import expr\n",
        "expr(\"(((count + 5) * 200) - 6) < count\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLbkpZ4gTzXy",
        "outputId": "e13f232f-3e70-478c-e8ba-4dc2148d1445"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['_c0', '_c1', '_c2']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "spark.read.format(\"csv\")\\\n",
        ".load(\"2015-summary.csv\")\\\n",
        ".columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8zzljqAUCNO"
      },
      "source": [
        "\n",
        "## RECORDS AND ROWS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFrzcg_fT-wo",
        "outputId": "1d78c354-4ce5-45db-df25-a46d7ea72571"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Row(DEST_COUNTRY_NAME='DEST_COUNTRY_NAME', ORIGIN_COUNTRY_NAME='ORIGIN_COUNTRY_NAME', count=None)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "df.first()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkcdQKUSUOnQ"
      },
      "source": [
        "## CREATE ROWS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "8x3tGAbBUMUZ"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import Row\n",
        "myRow = Row(\"Hello\", None, 1, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "IFOHF_ynUZOj",
        "outputId": "912a701d-6adb-4aae-fab1-5e730680474c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "myRow[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ml2XUVtCUdIv",
        "outputId": "07eb0f08-bcdd-4f9d-8579-c24c57eae055"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "myRow[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlP5LnkzUsUf"
      },
      "source": [
        "## CREATING DATAFRAME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46q27RHzUyny",
        "outputId": "7dd5991d-182e-4e37-cb30-9df2d18544c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----+----+\n",
            "| col1|col2|col3|\n",
            "+-----+----+----+\n",
            "|Hello|null|   1|\n",
            "+-----+----+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import Row\n",
        "from pyspark.sql.types import StructField, StructType,\\\n",
        " StringType, LongType\n",
        "myManualSchema = StructType([\n",
        "StructField(\"col1\", StringType(), True),\n",
        "StructField(\"col2\", StringType(), True),\n",
        "StructField(\"col3\", LongType(), False)\n",
        "])\n",
        "myRow = Row(\"Hello\", None, 1)\n",
        "myDf = spark.createDataFrame([myRow], myManualSchema)\n",
        "myDf.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkOEcd4EVJC5"
      },
      "source": [
        "##SELECT & SELECTEXPR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfbKDm87VEQQ",
        "outputId": "5e4acc4e-6468-4088-f619-07f11198e685"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+\n",
            "|DEST_COUNTRY_NAME|\n",
            "+-----------------+\n",
            "|DEST_COUNTRY_NAME|\n",
            "|    United States|\n",
            "+-----------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.select(\"DEST_COUNTRY_NAME\").show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ouhkTblVUVa",
        "outputId": "ba4eff6e-1f22-40f3-83e8-e9daab996303"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-------------------+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|\n",
            "+-----------------+-------------------+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|\n",
            "|    United States|            Romania|\n",
            "+-----------------+-------------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.select(\n",
        "\"DEST_COUNTRY_NAME\",\n",
        "\"ORIGIN_COUNTRY_NAME\")\\\n",
        ".show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwmuGtUBVaDk",
        "outputId": "6dd17738-3682-4ddd-8021-185b609c3235"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-----------------+-----------------+\n",
            "|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|\n",
            "+-----------------+-----------------+-----------------+\n",
            "|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|\n",
            "|    United States|    United States|    United States|\n",
            "+-----------------+-----------------+-----------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import expr, col, column\n",
        "df.select(\n",
        "expr(\"DEST_COUNTRY_NAME\"),\n",
        "col(\"DEST_COUNTRY_NAME\"),\n",
        "column(\"DEST_COUNTRY_NAME\"))\\\n",
        ".show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pl5HEgQRVlrp",
        "outputId": "816d2186-20ca-4aa5-839f-a3ab93a1fd26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-----------------+\n",
            "|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|\n",
            "+-----------------+-----------------+\n",
            "|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|\n",
            "|    United States|    United States|\n",
            "+-----------------+-----------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.select(col(\"DEST_COUNTRY_NAME\"), \"DEST_COUNTRY_NAME\").show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FayMmKjkV11v",
        "outputId": "fe173d1c-9f47-4c4b-e7ec-fd264bc4ffc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+\n",
            "|      destination|\n",
            "+-----------------+\n",
            "|DEST_COUNTRY_NAME|\n",
            "|    United States|\n",
            "+-----------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.select(expr(\"DEST_COUNTRY_NAME AS destination\")).show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xv2uygxWmNR",
        "outputId": "33a2d111-0e7b-49b1-c0b4-d889f2dae3f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+\n",
            "|DEST_COUNTRY_NAME|\n",
            "+-----------------+\n",
            "|DEST_COUNTRY_NAME|\n",
            "|    United States|\n",
            "+-----------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.select(\n",
        "expr(\"DEST_COUNTRY_NAME as destination\").alias(\"DEST_COUNTRY_NAME\")\n",
        ").show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kihMypPPWqm0",
        "outputId": "d8a65c32-a3c2-4b85-c0e7-ccb43aa612d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-----------------+\n",
            "|    newColumnName|DEST_COUNTRY_NAME|\n",
            "+-----------------+-----------------+\n",
            "|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|\n",
            "|    United States|    United States|\n",
            "+-----------------+-----------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.selectExpr(\n",
        "\"DEST_COUNTRY_NAME as newColumnName\",\n",
        "\"DEST_COUNTRY_NAME\"\n",
        ").show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aiu1Et_fXBe8",
        "outputId": "0d4483b8-3e9d-4752-8955-67a80494e78f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-------------------+-----+-------------+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n",
            "+-----------------+-------------------+-----+-------------+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| null|        false|\n",
            "|    United States|            Romania|   15|        false|\n",
            "+-----------------+-------------------+-----+-------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.selectExpr(\n",
        "\"*\", \n",
        "\"(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry\" )\\\n",
        ".show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2HAO2wzXSBA",
        "outputId": "3d955fe5-8bb1-4444-81fa-4a6b08648c7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+---------------------------------+\n",
            "| avg(count)|count(DISTINCT DEST_COUNTRY_NAME)|\n",
            "+-----------+---------------------------------+\n",
            "|1770.765625|                              133|\n",
            "+-----------+---------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.selectExpr(\"avg(count)\", \"count(distinct(DEST_COUNTRY_NAME))\").show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxmQDVYLXbaK",
        "outputId": "5d08dfee-59c3-4989-b780-8e1142571b71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-------------------+-----+---+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|One|\n",
            "+-----------------+-------------------+-----+---+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| null|  1|\n",
            "|    United States|            Romania|   15|  1|\n",
            "+-----------------+-------------------+-----+---+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import lit\n",
        "df.select(\n",
        "expr(\"*\"),\n",
        "lit(1).alias(\"One\")\n",
        ").show(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M516JzWwXqr0"
      },
      "source": [
        "##ADDING COLUMNS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpitOpsFXliN",
        "outputId": "a3f6ad16-5fa6-4fdc-d94d-e89f73aec43a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-------------------+-----+---------+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|numberOne|\n",
            "+-----------------+-------------------+-----+---------+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| null|        1|\n",
            "|    United States|            Romania|   15|        1|\n",
            "+-----------------+-------------------+-----+---------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.withColumn(\"numberOne\", lit(1)).show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRzsrWrBX3Dk",
        "outputId": "1614ca61-269f-4f0e-c9c5-85b7db9b0c7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-------------------+-----+-------------+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n",
            "+-----------------+-------------------+-----+-------------+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| null|        false|\n",
            "|    United States|            Romania|   15|        false|\n",
            "+-----------------+-------------------+-----+-------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.withColumn(\n",
        "\"withinCountry\",\n",
        "expr(\"ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME\"))\\\n",
        ".show(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_hsWklnYAWa"
      },
      "source": [
        "##RENAMING COLUMNS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctTVD-yLX-Ej",
        "outputId": "8fe6070a-80cb-4b20-d22a-e57175ede504"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['dest', 'ORIGIN_COUNTRY_NAME', 'count']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "df.withColumnRenamed(\"DEST_COUNTRY_NAME\", \"dest\").columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAkj2T1cYKIO",
        "outputId": "05b19db6-358a-479b-d52f-3ca1d2392680"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-------------------+-----+---------------------+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|This Long Column-Name|\n",
            "+-----------------+-------------------+-----+---------------------+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| null|  ORIGIN_COUNTRY_NAME|\n",
            "|    United States|            Romania|   15|              Romania|\n",
            "+-----------------+-------------------+-----+---------------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dfWithLongColName = df\\\n",
        ".withColumn(\n",
        "\"This Long Column-Name\",\n",
        "expr(\"ORIGIN_COUNTRY_NAME\"))\n",
        "dfWithLongColName.show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QaAgCES5YbEs",
        "outputId": "f5bdf310-2ebc-45fd-81ad-fb7c13c08dd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------+-------------------+\n",
            "|This Long Column-Name|            new col|\n",
            "+---------------------+-------------------+\n",
            "|  ORIGIN_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|\n",
            "|              Romania|            Romania|\n",
            "+---------------------+-------------------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dfWithLongColName\\\n",
        ".selectExpr(\n",
        "\"`This Long Column-Name`\",\n",
        "\"`This Long Column-Name` as `new col`\" )\\\n",
        ".show(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqzDfDoNYsGx",
        "outputId": "db78bd56-5647-4c82-969e-a351151d5c84"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This Long Column-Name']"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "dfWithLongColName.select(expr(\"`This Long Column-Name`\")).columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AT3AkAcYvVX",
        "outputId": "29f9ea34-423e-4006-d063-0bfeae4f273d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['DEST_COUNTRY_NAME', 'count']"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "df.drop(\"ORIGIN_COUNTRY_NAME\").columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JX0UkeJQY27s",
        "outputId": "b06dbf17-b51c-46ad-e930-3c67fc991ba8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[count: bigint, This Long Column-Name: string]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "dfWithLongColName.drop(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgyxndtQZFpg"
      },
      "source": [
        "##CHANGING COLUMNS's TYPE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DRIS4ccY-ij",
        "outputId": "c0e9e537-d697-48a4-a751-dfc5fce8a634"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
            " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
            " |-- count: long (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VT8NhdGnZSLF",
        "outputId": "def46eaf-9440-4ae4-c674-b9bf09f91f12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
            " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
            " |-- count: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.withColumn(\"count\",col(\"count\").cast(\"int\")).printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwDvSg8-ZfTi"
      },
      "source": [
        "##FILTERING ROWS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "LojlYxlMZbYh"
      },
      "outputs": [],
      "source": [
        "colCondition = df.filter(col(\"count\") < 2).take(2)\n",
        "conditional = df.where(\"count < 2\").take(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWs62882Zq5O",
        "outputId": "24214eea-e3d0-4a1f-b302-6f2e34dbafc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1), Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Singapore', count=1)]\n",
            "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1), Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Singapore', count=1)]\n"
          ]
        }
      ],
      "source": [
        "print(colCondition)\n",
        "print(conditional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iooesUrOZuEU",
        "outputId": "82ba7f84-8749-4ad2-e14a-212920ca9339"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+-----------------+-------------------+-----+\n",
            "|    United States|          Singapore|    1|\n",
            "|          Moldova|      United States|    1|\n",
            "+-----------------+-------------------+-----+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.where(col(\"count\") < 2)\\\n",
        ".where(col(\"ORIGIN_COUNTRY_NAME\") != \"Croatia\")\\\n",
        ".show(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3e5A-YiwaDnX"
      },
      "source": [
        "##GETTING UNIQUE ROWS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RuD9BMUPaAek",
        "outputId": "1c4cbd83-a784-4297-8b8d-3715d5c8ba59"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "257"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "df.select(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\").count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtR9s1zKaNi4",
        "outputId": "e2eb2772-e749-4fd8-965a-d445ecf27f87"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "126"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "df.select(\"ORIGIN_COUNTRY_NAME\").distinct().count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDO5g-YjaYcK"
      },
      "source": [
        "##SORTING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yT-PXIraRbv",
        "outputId": "0f335701-7c9d-478e-8758-27699f44b1e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-------------------+-----+\n",
            "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+--------------------+-------------------+-----+\n",
            "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| null|\n",
            "|       United States|          Singapore|    1|\n",
            "|               Malta|      United States|    1|\n",
            "|Saint Vincent and...|      United States|    1|\n",
            "|       United States|            Croatia|    1|\n",
            "+--------------------+-------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n",
            "+-----------------+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+-----------------+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| null|\n",
            "|     Burkina Faso|      United States|    1|\n",
            "|    Cote d'Ivoire|      United States|    1|\n",
            "|           Cyprus|      United States|    1|\n",
            "|         Djibouti|      United States|    1|\n",
            "+-----------------+-------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n",
            "+-----------------+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+-----------------+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| null|\n",
            "|     Burkina Faso|      United States|    1|\n",
            "|    Cote d'Ivoire|      United States|    1|\n",
            "|           Cyprus|      United States|    1|\n",
            "|         Djibouti|      United States|    1|\n",
            "+-----------------+-------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.sort(\"count\").show(5)\n",
        "df.orderBy(\"count\", \"DEST_COUNTRY_NAME\").show(5)\n",
        "df.orderBy(col(\"count\"), col(\"DEST_COUNTRY_NAME\")).show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6zCSqyxakYE",
        "outputId": "2dba5c3a-3552-4afe-e099-317a19262a37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+-----------------+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| null|\n",
            "|            Malta|      United States|    1|\n",
            "+-----------------+-------------------+-----+\n",
            "only showing top 2 rows\n",
            "\n",
            "+-----------------+-------------------+------+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n",
            "+-----------------+-------------------+------+\n",
            "|    United States|      United States|370002|\n",
            "|    United States|             Canada|  8483|\n",
            "+-----------------+-------------------+------+\n",
            "only showing top 2 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import desc, asc\n",
        "df.orderBy(expr(\"count desc\")).show(2)\n",
        "df.orderBy(desc(col(\"count\")), asc(col(\"DEST_COUNTRY_NAME\"))).show(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIh-yU4ua98j"
      },
      "source": [
        "##LIMIT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7rHCcvTatiN",
        "outputId": "4ef50ce2-d633-40e4-e121-b45a9b6be7de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+-----------------+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| null|\n",
            "|    United States|            Romania|   15|\n",
            "|    United States|            Croatia|    1|\n",
            "|    United States|            Ireland|  344|\n",
            "|            Egypt|      United States|   15|\n",
            "+-----------------+-------------------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.limit(5).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4OEe20jaz__",
        "outputId": "981a8a39-97fc-412f-ab4f-4053b262f664"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-------------------+-----+\n",
            "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+--------------------+-------------------+-----+\n",
            "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| null|\n",
            "|       United States|          Singapore|    1|\n",
            "|Saint Vincent and...|      United States|    1|\n",
            "|               Malta|      United States|    1|\n",
            "|       United States|            Croatia|    1|\n",
            "|       United States|          Gibraltar|    1|\n",
            "+--------------------+-------------------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.orderBy(expr(\"count desc\")).limit(6).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RH0SR9da_2t"
      },
      "source": [
        "##REPARTITION AND COALESCE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjOhQdVXa3oN",
        "outputId": "9317ec6c-2537-4a41-f85c-78d482c9c5b8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "df.rdd.getNumPartitions()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTHfjfYNbI-y",
        "outputId": "9feff3ad-4a63-4281-de41-6e4c9d40daa1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "df.repartition(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBeKeQHNbSDK",
        "outputId": "35998ece-e1bf-425a-c05e-0b4b989b511e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "df.repartition(col(\"DEST_COUNTRY_NAME\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IT8WH34IbWVV",
        "outputId": "d36c656d-53f6-4a7e-a05b-161678904cf7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "df.repartition(5, col(\"DEST_COUNTRY_NAME\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60Yle-AbbZ4l",
        "outputId": "acb34835-2f56-4407-9567-4c66ae111e2b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "df.repartition(5, col(\"DEST_COUNTRY_NAME\")).coalesce(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQ2gcpwGb_wa"
      },
      "source": [
        "##AGGREGATION FUNCTIONS\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_RMmei5cLeg"
      },
      "source": [
        "1)count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9x2-EWWbeYP",
        "outputId": "d4d95778-3083-4b0f-8257-e9acb3ca56b8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "257"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "df.select(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\").count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkes69nucmFe"
      },
      "source": [
        "2)CountDistinct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c429_u55cfep",
        "outputId": "6a83a2b9-6610-4339-bd46-ee1329b656de"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "126"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "df.select(\"ORIGIN_COUNTRY_NAME\").distinct().count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3C4XG9_csGp"
      },
      "source": [
        "3)first and last"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMHxatRAcjub",
        "outputId": "183912ec-03a9-465c-ac85-19030ab294ae"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Row(DEST_COUNTRY_NAME='DEST_COUNTRY_NAME', ORIGIN_COUNTRY_NAME='ORIGIN_COUNTRY_NAME', count=None)"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "df.first()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuWS_JK8c0Dc",
        "outputId": "d61d9906-44ed-47fd-b22e-90bf65883a47"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(DEST_COUNTRY_NAME='Greece', ORIGIN_COUNTRY_NAME='United States', count=30)]"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "df.tail(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rK57-nTfdQRL"
      },
      "source": [
        "4)min and max"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHXPKqL_dnMo",
        "outputId": "b000a32d-a9fc-4f60-c32e-53105e48e278"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(min(count)=1)]"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "from pyspark.sql.functions import min\n",
        "df.select(min(\"count\")).take(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQssr1KRc1c3",
        "outputId": "c0c965f0-479a-44e9-918b-de32932b05b7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(max(count)=370002)]"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "from pyspark.sql.functions import max\n",
        "df.select(max(\"count\")).take(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpKKLd-OdtiI"
      },
      "source": [
        "5)sum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFWSknxkdkXC",
        "outputId": "1590c7c3-1fa0-471a-b00e-5fa9d3c397f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+\n",
            "|sum(count)|\n",
            "+----------+\n",
            "|    453316|\n",
            "+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import sum\n",
        "df.select(sum(\"count\")).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_bdIbrbfNOn"
      },
      "source": [
        "6)sumDistinct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ul1i3Q6ifMw-",
        "outputId": "aff2f3ee-4692-4b84-c864-3646fff0b185"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+\n",
            "|sum(DISTINCT count)|\n",
            "+-------------------+\n",
            "|             450718|\n",
            "+-------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import sum_distinct\n",
        "\n",
        "df.select(sum_distinct(\"count\")).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBDg-f_geydT",
        "outputId": "3fd1b9e2-80ae-48a4-b5e7-500969512f31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+\n",
            "| avg(count)|\n",
            "+-----------+\n",
            "|1770.765625|\n",
            "+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import *\n",
        "\n",
        "df.select(avg(\"count\")).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHF3w-64fqFl"
      },
      "source": [
        "7)grouping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "Rjrs8kPOfio-"
      },
      "outputs": [],
      "source": [
        "df1= df.groupBy(\"DEST_COUNTRY_NAME\").count()\n",
        "df2 = df.groupBy(\"ORIGIN_COUNTRY_NAME\").count()\n",
        "df3 = df.groupBy(\"count\").count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lh_yfPV8f9ck",
        "outputId": "fdb75599-c984-4af8-d740-c4ca2f07fcfd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-----+\n",
            "|DEST_COUNTRY_NAME|count|\n",
            "+-----------------+-----+\n",
            "|         Anguilla|    1|\n",
            "|           Russia|    1|\n",
            "|         Paraguay|    1|\n",
            "+-----------------+-----+\n",
            "only showing top 3 rows\n",
            "\n",
            "+-------------------+-----+\n",
            "|ORIGIN_COUNTRY_NAME|count|\n",
            "+-------------------+-----+\n",
            "|           Paraguay|    1|\n",
            "|             Russia|    1|\n",
            "|           Anguilla|    1|\n",
            "+-------------------+-----+\n",
            "only showing top 3 rows\n",
            "\n",
            "+-----+-----+\n",
            "|count|count|\n",
            "+-----+-----+\n",
            "|   26|    2|\n",
            "|  442|    1|\n",
            "|   19|    3|\n",
            "+-----+-----+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df1.show(3)\n",
        "df2.show(3)\n",
        "df3.show(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVFr3xFDgXoB"
      },
      "source": [
        "8)Window Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIYDHB3ZgBma",
        "outputId": "0fdd1f67-17cc-4796-89ed-c450fca4c0e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- employee_name: string (nullable = true)\n",
            " |-- department: string (nullable = true)\n",
            " |-- salary: long (nullable = true)\n",
            "\n",
            "+-------------+----------+------+\n",
            "|employee_name|department|salary|\n",
            "+-------------+----------+------+\n",
            "|James        |Sales     |3000  |\n",
            "|Michael      |Sales     |4600  |\n",
            "|Robert       |Sales     |4100  |\n",
            "|Maria        |Finance   |3000  |\n",
            "|James        |Sales     |3000  |\n",
            "|Scott        |Finance   |3300  |\n",
            "|Jen          |Finance   |3900  |\n",
            "|Jeff         |Marketing |3000  |\n",
            "|Kumar        |Marketing |2000  |\n",
            "|Saif         |Sales     |4100  |\n",
            "+-------------+----------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "simpleData = ((\"James\", \"Sales\", 3000), \\\n",
        "    (\"Michael\", \"Sales\", 4600),  \\\n",
        "    (\"Robert\", \"Sales\", 4100),   \\\n",
        "    (\"Maria\", \"Finance\", 3000),  \\\n",
        "    (\"James\", \"Sales\", 3000),    \\\n",
        "    (\"Scott\", \"Finance\", 3300),  \\\n",
        "    (\"Jen\", \"Finance\", 3900),    \\\n",
        "    (\"Jeff\", \"Marketing\", 3000), \\\n",
        "    (\"Kumar\", \"Marketing\", 2000),\\\n",
        "    (\"Saif\", \"Sales\", 4100) \\\n",
        "  )\n",
        " \n",
        "columns= [\"employee_name\", \"department\", \"salary\"]\n",
        "df = spark.createDataFrame(data = simpleData, schema = columns)\n",
        "df.printSchema()\n",
        "df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3_wRTtIlJBr",
        "outputId": "01b27c99-a647-4295-ce7e-46098a8c1318"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------+------+----------+\n",
            "|employee_name|department|salary|row_number|\n",
            "+-------------+----------+------+----------+\n",
            "|Maria        |Finance   |3000  |1         |\n",
            "|Scott        |Finance   |3300  |2         |\n",
            "|Jen          |Finance   |3900  |3         |\n",
            "|Kumar        |Marketing |2000  |1         |\n",
            "|Jeff         |Marketing |3000  |2         |\n",
            "|James        |Sales     |3000  |1         |\n",
            "|James        |Sales     |3000  |2         |\n",
            "|Robert       |Sales     |4100  |3         |\n",
            "|Saif         |Sales     |4100  |4         |\n",
            "|Michael      |Sales     |4600  |5         |\n",
            "+-------------+----------+------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import row_number\n",
        "windowSpec  = Window.partitionBy(\"department\").orderBy(\"salary\")\n",
        "\n",
        "df.withColumn(\"row_number\",row_number().over(windowSpec)) \\\n",
        "    .show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqzqSB8mlSLi",
        "outputId": "34393c52-2d1f-4cbd-ea8e-ab833517182a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------+------+----+\n",
            "|employee_name|department|salary|rank|\n",
            "+-------------+----------+------+----+\n",
            "|        Maria|   Finance|  3000|   1|\n",
            "|        Scott|   Finance|  3300|   2|\n",
            "|          Jen|   Finance|  3900|   3|\n",
            "|        Kumar| Marketing|  2000|   1|\n",
            "|         Jeff| Marketing|  3000|   2|\n",
            "|        James|     Sales|  3000|   1|\n",
            "|        James|     Sales|  3000|   1|\n",
            "|       Robert|     Sales|  4100|   3|\n",
            "|         Saif|     Sales|  4100|   3|\n",
            "|      Michael|     Sales|  4600|   5|\n",
            "+-------------+----------+------+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import rank\n",
        "df.withColumn(\"rank\",rank().over(windowSpec)) \\\n",
        "    .show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCLJ8yyWlTBR",
        "outputId": "93a02587-8b32-44c7-a8f8-79f1f238170a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------+------+----------+\n",
            "|employee_name|department|salary|dense_rank|\n",
            "+-------------+----------+------+----------+\n",
            "|        Maria|   Finance|  3000|         1|\n",
            "|        Scott|   Finance|  3300|         2|\n",
            "|          Jen|   Finance|  3900|         3|\n",
            "|        Kumar| Marketing|  2000|         1|\n",
            "|         Jeff| Marketing|  3000|         2|\n",
            "|        James|     Sales|  3000|         1|\n",
            "|        James|     Sales|  3000|         1|\n",
            "|       Robert|     Sales|  4100|         2|\n",
            "|         Saif|     Sales|  4100|         2|\n",
            "|      Michael|     Sales|  4600|         3|\n",
            "+-------------+----------+------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import dense_rank\n",
        "df.withColumn(\"dense_rank\",dense_rank().over(windowSpec)) \\\n",
        "    .show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFDTX8oPlXnT",
        "outputId": "c2375d83-9a86-4a4a-9d10-d663463b746d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------+------+------------+\n",
            "|employee_name|department|salary|percent_rank|\n",
            "+-------------+----------+------+------------+\n",
            "|        Maria|   Finance|  3000|         0.0|\n",
            "|        Scott|   Finance|  3300|         0.5|\n",
            "|          Jen|   Finance|  3900|         1.0|\n",
            "|        Kumar| Marketing|  2000|         0.0|\n",
            "|         Jeff| Marketing|  3000|         1.0|\n",
            "|        James|     Sales|  3000|         0.0|\n",
            "|        James|     Sales|  3000|         0.0|\n",
            "|       Robert|     Sales|  4100|         0.5|\n",
            "|         Saif|     Sales|  4100|         0.5|\n",
            "|      Michael|     Sales|  4600|         1.0|\n",
            "+-------------+----------+------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import percent_rank\n",
        "df.withColumn(\"percent_rank\",percent_rank().over(windowSpec)) \\\n",
        "    .show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZbKgWotla1m",
        "outputId": "80b7dc14-ab3e-4a00-e3a6-27666c82bb35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------+------+-----+\n",
            "|employee_name|department|salary|ntile|\n",
            "+-------------+----------+------+-----+\n",
            "|        Maria|   Finance|  3000|    1|\n",
            "|        Scott|   Finance|  3300|    1|\n",
            "|          Jen|   Finance|  3900|    2|\n",
            "|        Kumar| Marketing|  2000|    1|\n",
            "|         Jeff| Marketing|  3000|    2|\n",
            "|        James|     Sales|  3000|    1|\n",
            "|        James|     Sales|  3000|    1|\n",
            "|       Robert|     Sales|  4100|    1|\n",
            "|         Saif|     Sales|  4100|    2|\n",
            "|      Michael|     Sales|  4600|    2|\n",
            "+-------------+----------+------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import ntile\n",
        "df.withColumn(\"ntile\",ntile(2).over(windowSpec)) \\\n",
        "    .show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-koDVOildWc",
        "outputId": "edf063ab-927b-4be1-d205-1b504217795f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------+------+------------------+\n",
            "|employee_name|department|salary|         cume_dist|\n",
            "+-------------+----------+------+------------------+\n",
            "|        Maria|   Finance|  3000|0.3333333333333333|\n",
            "|        Scott|   Finance|  3300|0.6666666666666666|\n",
            "|          Jen|   Finance|  3900|               1.0|\n",
            "|        Kumar| Marketing|  2000|               0.5|\n",
            "|         Jeff| Marketing|  3000|               1.0|\n",
            "|        James|     Sales|  3000|               0.4|\n",
            "|        James|     Sales|  3000|               0.4|\n",
            "|       Robert|     Sales|  4100|               0.8|\n",
            "|         Saif|     Sales|  4100|               0.8|\n",
            "|      Michael|     Sales|  4600|               1.0|\n",
            "+-------------+----------+------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import cume_dist    \n",
        "df.withColumn(\"cume_dist\",cume_dist().over(windowSpec)) \\\n",
        "   .show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHrfefSmlgq_",
        "outputId": "f8b7d28a-9dcd-46e5-d354-060164cdbaa6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------+------+----+\n",
            "|employee_name|department|salary| lag|\n",
            "+-------------+----------+------+----+\n",
            "|        Maria|   Finance|  3000|null|\n",
            "|        Scott|   Finance|  3300|null|\n",
            "|          Jen|   Finance|  3900|3000|\n",
            "|        Kumar| Marketing|  2000|null|\n",
            "|         Jeff| Marketing|  3000|null|\n",
            "|        James|     Sales|  3000|null|\n",
            "|        James|     Sales|  3000|null|\n",
            "|       Robert|     Sales|  4100|3000|\n",
            "|         Saif|     Sales|  4100|3000|\n",
            "|      Michael|     Sales|  4600|4100|\n",
            "+-------------+----------+------+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import lag    \n",
        "df.withColumn(\"lag\",lag(\"salary\",2).over(windowSpec)) \\\n",
        "      .show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YH6QeiyZlj07",
        "outputId": "2c83b833-e4f1-4f9b-9608-ee70fca8165a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------+------+----+\n",
            "|employee_name|department|salary|lead|\n",
            "+-------------+----------+------+----+\n",
            "|        Maria|   Finance|  3000|3900|\n",
            "|        Scott|   Finance|  3300|null|\n",
            "|          Jen|   Finance|  3900|null|\n",
            "|        Kumar| Marketing|  2000|null|\n",
            "|         Jeff| Marketing|  3000|null|\n",
            "|        James|     Sales|  3000|4100|\n",
            "|        James|     Sales|  3000|4100|\n",
            "|       Robert|     Sales|  4100|4600|\n",
            "|         Saif|     Sales|  4100|null|\n",
            "|      Michael|     Sales|  4600|null|\n",
            "+-------------+----------+------+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import lead    \n",
        "df.withColumn(\"lead\",lead(\"salary\",2).over(windowSpec)) \\\n",
        "    .show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Od5Zd9ilmXm",
        "outputId": "b7322f5d-637e-487a-c239-0d859083a67a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------+-----+----+----+\n",
            "|department|   avg|  sum| min| max|\n",
            "+----------+------+-----+----+----+\n",
            "|   Finance|3400.0|10200|3000|3900|\n",
            "| Marketing|2500.0| 5000|2000|3000|\n",
            "|     Sales|3760.0|18800|3000|4600|\n",
            "+----------+------+-----+----+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "windowSpecAgg  = Window.partitionBy(\"department\")\n",
        "from pyspark.sql.functions import col,avg,sum,min,max,row_number \n",
        "df.withColumn(\"row\",row_number().over(windowSpec)) \\\n",
        "  .withColumn(\"avg\", avg(col(\"salary\")).over(windowSpecAgg)) \\\n",
        "  .withColumn(\"sum\", sum(col(\"salary\")).over(windowSpecAgg)) \\\n",
        "  .withColumn(\"min\", min(col(\"salary\")).over(windowSpecAgg)) \\\n",
        "  .withColumn(\"max\", max(col(\"salary\")).over(windowSpecAgg)) \\\n",
        "  .where(col(\"row\")==1).select(\"department\",\"avg\",\"sum\",\"min\",\"max\") \\\n",
        "  .show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7OuNwI5iIP-"
      },
      "source": [
        "# Day 4 (chapter 8)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qc_NRnx8XL8Q"
      },
      "source": [
        "## 1. Joins"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"2015-summary.csv\")\n",
        "df2 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"2014-summary.csv\")"
      ],
      "metadata": {
        "id": "_zLANGaL3O-u"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lr_vHvC6WO15"
      },
      "source": [
        "###1. Inner Join:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGuLhJ6JVEWF",
        "outputId": "c34cc3fc-c394-4192-eefb-e81537bdfc66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-------------------+-----+--------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count| ORIGIN_COUNTRY_NAME|count|\n",
            "+-----------------+-------------------+-----+--------------------+-----+\n",
            "|    United States|            Romania|   15|               Haiti|  193|\n",
            "|    United States|            Romania|   15|Saint Kitts and N...|  123|\n",
            "|    United States|            Romania|   15|       French Guiana|    4|\n",
            "|    United States|            Romania|   15|             Bolivia|   14|\n",
            "|    United States|            Romania|   15| Trinidad and Tobago|  175|\n",
            "+-----------------+-------------------+-----+--------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "inner_join = df.join(df2, [\"DEST_COUNTRY_NAME\"], \"inner\")\n",
        "inner_join.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8OBtahAWbkE"
      },
      "source": [
        "###2. Left Outer Join:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6gS0LvZ2_3u",
        "outputId": "56ec6a05-c1d4-4b55-e396-61f8eec8df71"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['DEST_COUNTRY_NAME', 'ORIGIN_COUNTRY_NAME', 'count']"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYZWTVrmWaZx",
        "outputId": "7bd64eff-307e-4204-acdd-ba0ed9ae6fbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-------------------+-----+--------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count| ORIGIN_COUNTRY_NAME|count|\n",
            "+-----------------+-------------------+-----+--------------------+-----+\n",
            "|    United States|            Romania|   15|               Haiti|  193|\n",
            "|    United States|            Romania|   15|Saint Kitts and N...|  123|\n",
            "|    United States|            Romania|   15|       French Guiana|    4|\n",
            "|    United States|            Romania|   15|             Bolivia|   14|\n",
            "|    United States|            Romania|   15| Trinidad and Tobago|  175|\n",
            "+-----------------+-------------------+-----+--------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "left_outer_join = df.join(df2, [\"DEST_COUNTRY_NAME\"], \"left_outer\")\n",
        "left_outer_join.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PO7PndDOWoIC"
      },
      "source": [
        "###3. Right Outer Join:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dm5ch6yWqzo",
        "outputId": "c61faab8-5059-4e56-80a0-ac8830dc70ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+--------------------+-----+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME| ORIGIN_COUNTRY_NAME|count|ORIGIN_COUNTRY_NAME|count|\n",
            "+-----------------+--------------------+-----+-------------------+-----+\n",
            "|    United States|               Haiti|  225|       Saint Martin|    1|\n",
            "|    United States|Saint Kitts and N...|  145|       Saint Martin|    1|\n",
            "|    United States|             Bolivia|   13|       Saint Martin|    1|\n",
            "|    United States| Trinidad and Tobago|  217|       Saint Martin|    1|\n",
            "|    United States|             Namibia|    1|       Saint Martin|    1|\n",
            "+-----------------+--------------------+-----+-------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "right_outer_join = df.join(df2, [\"DEST_COUNTRY_NAME\"], \"right_outer\")\n",
        "right_outer_join.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtQ3tgkiW1wX"
      },
      "source": [
        "###4. Full Outer Join:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5dpSC15W0ih",
        "outputId": "45237eb8-373e-4989-f161-e62d0aa628aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+-------------------+-----+-------------------+-----+\n",
            "|  DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|ORIGIN_COUNTRY_NAME|count|\n",
            "+-------------------+-------------------+-----+-------------------+-----+\n",
            "|            Algeria|      United States|    4|      United States|    4|\n",
            "|             Angola|      United States|   15|      United States|   15|\n",
            "|           Anguilla|      United States|   41|      United States|   41|\n",
            "|Antigua and Barbuda|      United States|  126|      United States|  126|\n",
            "|          Argentina|      United States|  180|      United States|  180|\n",
            "+-------------------+-------------------+-----+-------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "full_outer_join = df.join(df, [\"DEST_COUNTRY_NAME\"], \"full_outer\")\n",
        "full_outer_join.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HS0LsJZVXgY9"
      },
      "source": [
        "###5. Left Semi Join:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAevzTZQXfsT",
        "outputId": "495da4e8-b5a9-468d-d49b-8f02933e378b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+-----------------+-------------------+-----+\n",
            "|    United States|            Romania|   15|\n",
            "|    United States|            Croatia|    1|\n",
            "|    United States|            Ireland|  344|\n",
            "|            Egypt|      United States|   15|\n",
            "|    United States|              India|   62|\n",
            "+-----------------+-------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "left_semi_join = df.join(df, df.DEST_COUNTRY_NAME == df.DEST_COUNTRY_NAME, \"leftsemi\")\n",
        "left_semi_join.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKNv70ouZFHK"
      },
      "source": [
        "###6. Left Anti Join:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fW4Q51djY8R_",
        "outputId": "4267695f-73a4-4af6-f54d-d8869a3fc9d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+-----------------+-------------------+-----+\n",
            "+-----------------+-------------------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "left_anti_join = df2.join(df2, df2.DEST_COUNTRY_NAME == df2.DEST_COUNTRY_NAME, \"leftanti\")\n",
        "left_anti_join.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HurtU23ZWdO"
      },
      "source": [
        "###7. Cross Join:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjA88CVwZWDk",
        "outputId": "d7a503ad-366d-4fe7-a546-87662604b1f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-------------------+-----+-----------------+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+-----------------+-------------------+-----+-----------------+-------------------+-----+\n",
            "|    United States|       Saint Martin|    1|    United States|            Romania|   15|\n",
            "|    United States|            Romania|   12|    United States|            Romania|   15|\n",
            "|    United States|            Croatia|    2|    United States|            Romania|   15|\n",
            "|    United States|            Ireland|  291|    United States|            Romania|   15|\n",
            "|    United States|              India|   62|    United States|            Romania|   15|\n",
            "+-----------------+-------------------+-----+-----------------+-------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "cross_join = df2.crossJoin(df)\n",
        "cross_join.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVCqViucZi3q"
      },
      "source": [
        "## Handling Duplicate column names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmSkwOwgZqUX"
      },
      "source": [
        "**1:** Renaming the columns: You can use the withColumnRenamed method to rename the duplicate columns before the join. For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5vKTTTeZgCn",
        "outputId": "61a12f4e-f63c-468f-83ea-c9f64bd06f8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME_1|ORIGIN_COUNTRY_NAME|count|\n",
            "+-------------------+-------------------+-----+\n",
            "|      United States|            Romania|   15|\n",
            "|      United States|            Croatia|    1|\n",
            "|      United States|            Ireland|  344|\n",
            "|              Egypt|      United States|   15|\n",
            "|      United States|              India|   62|\n",
            "+-------------------+-------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n",
            "+-------------------+-------------------+-----+\n",
            "|DEST_COUNTRY_NAME_2|ORIGIN_COUNTRY_NAME|count|\n",
            "+-------------------+-------------------+-----+\n",
            "|      United States|       Saint Martin|    1|\n",
            "|      United States|            Romania|   12|\n",
            "|      United States|            Croatia|    2|\n",
            "|      United States|            Ireland|  291|\n",
            "|      United States|              India|   62|\n",
            "+-------------------+-------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.withColumnRenamed(\"DEST_COUNTRY_NAME\", \"DEST_COUNTRY_NAME_1\").show(5)\n",
        "df2.withColumnRenamed(\"DEST_COUNTRY_NAME\", \"DEST_COUNTRY_NAME_2\").show(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZ8gNgY6cJUS"
      },
      "source": [
        "**2:** Using the as keyword: When selecting columns, you can use the alias method or the as keyword to give a new name to the duplicate column. For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMTS5jWGcfrg",
        "outputId": "ade17d1e-58da-4594-8785-385714d4cd83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+\n",
            "|DEST_COUNTRY_NAME_1|\n",
            "+-------------------+\n",
            "|      United States|\n",
            "|      United States|\n",
            "|      United States|\n",
            "|              Egypt|\n",
            "|      United States|\n",
            "+-------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+-------------------+\n",
            "|DEST_COUNTRY_NAME_1|\n",
            "+-------------------+\n",
            "|      United States|\n",
            "|      United States|\n",
            "|      United States|\n",
            "|      United States|\n",
            "|      United States|\n",
            "+-------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.select(df[\"DEST_COUNTRY_NAME\"].alias(\"DEST_COUNTRY_NAME_1\")).show(5)\n",
        "df2.selectExpr(\"DEST_COUNTRY_NAME as DEST_COUNTRY_NAME_1\").show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzVZeVVVdUWJ"
      },
      "source": [
        "**3:** Using the withColumn method: You can use the withColumn method to add a new column with a new name, and then drop the original column. For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrPIZ8ftdbBw",
        "outputId": "17f95d38-764e-49d8-c451-566e37084aa2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+-----+-------------------+\n",
            "|ORIGIN_COUNTRY_NAME|count|DEST_COUNTRY_NAME_1|\n",
            "+-------------------+-----+-------------------+\n",
            "|            Romania|   15|      United States|\n",
            "|            Croatia|    1|      United States|\n",
            "|            Ireland|  344|      United States|\n",
            "|      United States|   15|              Egypt|\n",
            "|              India|   62|      United States|\n",
            "+-------------------+-----+-------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "summ= df.withColumn(\"DEST_COUNTRY_NAME_1\", df[\"DEST_COUNTRY_NAME\"])\n",
        "summ.drop(\"DEST_COUNTRY_NAME\").show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBAvL3-udx1i"
      },
      "source": [
        "**4:** Using the select method: You can use the select method to select only the columns you need from the DataFrame, which will remove the duplicate columns. For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1Yf7lYOdxFP",
        "outputId": "d635f700-a120-4958-b885-9e14222d8e38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+-----+-------------------+\n",
            "|DEST_COUNTRY_NAME_1|count|ORIGIN_COUNTRY_NAME|\n",
            "+-------------------+-----+-------------------+\n",
            "|      United States|   15|            Romania|\n",
            "|      United States|    1|            Croatia|\n",
            "|      United States|  344|            Ireland|\n",
            "|              Egypt|   15|      United States|\n",
            "|      United States|   62|              India|\n",
            "+-------------------+-----+-------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "summ.select(\"DEST_COUNTRY_NAME_1\",\"count\",\"ORIGIN_COUNTRY_NAME\").show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCNGqgs4eSz7"
      },
      "source": [
        "**5:** Using the drop method: You can use the drop method to drop duplicate columns after join the dataframe. For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clbtbvCpeR_b",
        "outputId": "62e4d346-4405-4fc9-bde2-2bbea2d6cc6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+-----+-------------------+--------------------+-----+\n",
            "|ORIGIN_COUNTRY_NAME|count|DEST_COUNTRY_NAME_1| ORIGIN_COUNTRY_NAME|count|\n",
            "+-------------------+-----+-------------------+--------------------+-----+\n",
            "|            Romania|   15|      United States|               Haiti|  193|\n",
            "|            Romania|   15|      United States|Saint Kitts and N...|  123|\n",
            "|            Romania|   15|      United States|       French Guiana|    4|\n",
            "|            Romania|   15|      United States|             Bolivia|   14|\n",
            "|            Romania|   15|      United States| Trinidad and Tobago|  175|\n",
            "+-------------------+-----+-------------------+--------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "sum = summ.join(df2, df.DEST_COUNTRY_NAME == df2.DEST_COUNTRY_NAME, \"inner\")\n",
        "sum.drop(\"DEST_COUNTRY_NAME\").show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUZRGGmthVL3"
      },
      "source": [
        "## 3. How spark performs joins"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grIc5rB5hm16"
      },
      "source": [
        "In PySpark, joins are performed by the join method on a DataFrame, which takes one or more DataFrames as arguments. The basic syntax for joining two DataFrames is as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_KJj4pfe_F4",
        "outputId": "39705af1-1b85-49ee-b6c3-eef3ebd12615"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-------------------+-----+-----------------+--------------------+-----+\n",
            "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|DEST_COUNTRY_NAME| ORIGIN_COUNTRY_NAME|count|\n",
            "+-----------------+-------------------+-----+-----------------+--------------------+-----+\n",
            "|    United States|            Romania|   15|    United States|               Haiti|  225|\n",
            "|    United States|            Romania|   15|    United States|Saint Kitts and N...|  145|\n",
            "|    United States|            Romania|   15|    United States|             Bolivia|   13|\n",
            "|    United States|            Romania|   15|    United States| Trinidad and Tobago|  217|\n",
            "|    United States|            Romania|   15|    United States|             Namibia|    1|\n",
            "+-----------------+-------------------+-----+-----------------+--------------------+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.join(df, df.DEST_COUNTRY_NAME == df.DEST_COUNTRY_NAME, \"inner\").show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6AGoPnBiQDN"
      },
      "source": [
        "# Day 5 (chapter 9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eruRH03UiW_I"
      },
      "source": [
        "## 1. Datasources\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w609il1aiZO2"
      },
      "source": [
        "### 1.1. Basics of reading data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRxTJs8PixTK"
      },
      "source": [
        "The most commonly used method for reading data in PySpark is the read method of the SparkSession object.\n",
        "\n",
        "Here is an example of how to read a CSV file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "n_7ZbwGuhzg7"
      },
      "outputs": [],
      "source": [
        "#sum1 = spark.read.csv(\"path/to/file.csv\", header=True, inferSchema=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhfVRd6Ii_1M"
      },
      "source": [
        "## 2. Basics of write data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcAsjG8DjPrI"
      },
      "source": [
        "The most commonly used method for writing data in PySpark is the write method of the DataFrame object.\n",
        "\n",
        "Here is an example of how to write a DataFrame to a CSV file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "J2v6yZq4jROs"
      },
      "outputs": [],
      "source": [
        "#df.write.csv(\"path/to/new_file.csv\", header=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyyyrvSijYJt"
      },
      "source": [
        "## 3. CSV files - reading, writing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "4Km1EOASmWZQ"
      },
      "outputs": [],
      "source": [
        "#df = spark.read.csv(\"path/to/file.csv\", header=True, inferSchema=True)\n",
        "\n",
        "#df.write.csv(\"path/to/new_file.csv\", header=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jW3zymlbmZND"
      },
      "source": [
        "## 4. Reading and writing json files\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "GGPD8YUsmcia"
      },
      "outputs": [],
      "source": [
        "#df = spark.read.json(\"path/to/file.json\")\n",
        "\n",
        "#df.write.json(\"path/to/new_file.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lZ5wC6emsyf"
      },
      "source": [
        "## 5. Parquet files - important"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MJrVh-rnYZ2"
      },
      "source": [
        "Parquet is a columnar storage format that is widely used in the Apache Hadoop ecosystem and is supported by many big data processing frameworks, including PySpark. There are several reasons why Parquet is important in PySpark:\n",
        "\n",
        "**Efficiency:** Parquet stores data in a columnar format, which means that only the required columns are read and processed, rather than reading and processing the entire row. This leads to significant performance improvements when working with large datasets.\n",
        "\n",
        "**Compression:** Parquet supports various compression algorithms, such as Snappy and Gzip, which can greatly reduce the storage space required for large datasets.\n",
        "\n",
        "**Schema evolution:** Parquet supports schema evolution, which means that a dataset's schema can be changed over time without having to rewrite the entire dataset. This is particularly useful when working with data that is constantly changing or evolving.\n",
        "\n",
        "**Predicate pushdown:** Parquet supports predicate pushdown, which means that filtering conditions can be pushed down to the storage layer, rather than being applied in the query layer. This leads to further performance improvements when working with large datasets.\n",
        "\n",
        "**Interoperability:** Parquet is an open standard, which means that it can be used with a wide variety of data processing frameworks, including PySpark, Hive, Pig, and Impala."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R82CbHienicc"
      },
      "source": [
        "## 6. Reading and Writing parquet files\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwkPifqwnkHa"
      },
      "source": [
        "To read and write parquet files in PySpark you can use the read.parquet() and write.parquet() methods respectively. Here is an example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "GwB9B39qm0F-"
      },
      "outputs": [],
      "source": [
        "#df = spark.read.parquet(\"path/to/file.parquet\")\n",
        "\n",
        "#df.write.parquet(\"path/to/new_file.parquet\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17fB78L2nsx4"
      },
      "source": [
        "## 7. orc - optional\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGzSC7kwn25g"
      },
      "source": [
        "ORC (Optimized Row Columnar) is a file format that is similar to Parquet and is also widely used in the Apache Hadoop ecosystem. Like Parquet, ORC is a columnar storage format that is designed to improve the performance and storage efficiency of big data processing frameworks, such as PySpark.\n",
        "\n",
        "Here are some of the key benefits of using ORC in PySpark:\n",
        "\n",
        "**Performance:** ORC stores data in a columnar format, which leads to significant performance improvements when working with large datasets.\n",
        "\n",
        "**Compression:** ORC supports various compression algorithms, such as Snappy, Zlib, and LZO, which can greatly reduce the storage space required for large datasets.\n",
        "\n",
        "**Schema evolution:** ORC supports schema evolution, which means that a dataset's schema can be changed over time without having to rewrite the entire dataset.\n",
        "\n",
        "**Predicate pushdown:** ORC supports predicate pushdown, which means that filtering conditions can be pushed down to the storage layer, rather than being applied in the query layer.\n",
        "\n",
        "**Interoperability:** ORC is an open standard, which means that it can be used with a wide variety of data processing frameworks, including PySpark, Hive, Pig, and Impala."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhlFQDOAoTEZ"
      },
      "source": [
        "## 8. Splittable File Types and Compression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jplwQSCGoUw4"
      },
      "source": [
        "When working with PySpark, you can specify the file type and compression algorithm when reading and writing data using the appropriate methods. For example, you can use the read.parquet() method to read a Parquet file and the write.avro() method to write an Avro file.\n",
        "\n",
        "Here is an example of reading a parquet file with snappy compression:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "ftNmrmqtn2Ha"
      },
      "outputs": [],
      "source": [
        "#df.write.format(\"parquet\").option(\"compression\", \"snappy\").save(\"path/to/new_file.parquet\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdgUI11jo5P5"
      },
      "source": [
        "## 9. Managing File size\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mLgOLUjpUbW"
      },
      "source": [
        "Managing file size in PySpark is an important consideration when working with large datasets. There are several strategies that can be used to manage the file size of your data, including:\n",
        "\n",
        "**Partitioning:** Partitioning is the process of dividing a large dataset into smaller, more manageable chunks. In PySpark, you can partition your data using the partitionBy method when writing data to a file. This allows you to split your data into multiple smaller files based on a specific column, such as a date or a category.\n",
        "\n",
        "**Compression:** As mentioned previously, compression is the process of reducing the size of a dataset. This can be done by using a compression algorithm, such as Snappy, Gzip, LZO, or Zlib, when reading or writing data in PySpark.\n",
        "\n",
        "**File format:** Choosing the appropriate file format for your data can also help manage file size. Columnar file formats, such as Parquet and ORC, are often more efficient than row-based formats, such as CSV or JSON, when working with large datasets because they require less storage space and are more easily compressible.\n",
        "\n",
        "**Filtering:** Filtering is the process of removing unnecessary data from your dataset. In PySpark, you can filter your data using the filter method to remove rows that do not meet a specific criteria. This can help reduce the file size of your data by removing unneeded information.\n",
        "\n",
        "**Sampling:** Sampling is the process of selecting a random subset of your data to work with. In PySpark, you can use the sample method to randomly select a certain percentage or number of rows from your dataset. This can be useful when working with large datasets as it allows you to work with a smaller, more manageable subset of your data while still getting an accurate representation of the whole dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiV5K74Gj7HK"
      },
      "source": [
        "# Day 6 (chapter 10)\n",
        "Entire chapter of Spark SQL\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlmPWZPOlc5K"
      },
      "source": [
        "Spark SQL is a module for structured data processing in Apache Spark that provides a programming interface for working with structured data using SQL (Structured Query Language). It allows you to run SQL queries on DataFrames and manipulate structured data using SQL-like syntax."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "ufb9owxVlew9"
      },
      "outputs": [],
      "source": [
        "df.createOrReplaceTempView(\"summary\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mcli4DDlqcg",
        "outputId": "d93676aa-5a79-44ca-8ee2-339da5a74b77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-------------------+-----+\n",
            "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
            "+--------------------+-------------------+-----+\n",
            "|       United States|            Romania|   15|\n",
            "|       United States|            Croatia|    1|\n",
            "|       United States|            Ireland|  344|\n",
            "|               Egypt|      United States|   15|\n",
            "|       United States|              India|   62|\n",
            "|       United States|          Singapore|    1|\n",
            "|       United States|            Grenada|   62|\n",
            "|          Costa Rica|      United States|  588|\n",
            "|             Senegal|      United States|   40|\n",
            "|             Moldova|      United States|    1|\n",
            "|       United States|       Sint Maarten|  325|\n",
            "|       United States|   Marshall Islands|   39|\n",
            "|              Guyana|      United States|   64|\n",
            "|               Malta|      United States|    1|\n",
            "|            Anguilla|      United States|   41|\n",
            "|             Bolivia|      United States|   30|\n",
            "|       United States|           Paraguay|    6|\n",
            "|             Algeria|      United States|    4|\n",
            "|Turks and Caicos ...|      United States|  230|\n",
            "|       United States|          Gibraltar|    1|\n",
            "+--------------------+-------------------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql('SELECT * FROM summary').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQIrgLbhpkHK"
      },
      "source": [
        "1. What is the total number of flights?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4U2yDFMylw0k",
        "outputId": "81040117-b5a2-4412-9347-59bc61893d77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+\n",
            "|ROWS_count|\n",
            "+----------+\n",
            "|       256|\n",
            "+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql('SELECT COUNT(*) AS ROWS_count FROM summary').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUJiRJXuoYlk"
      },
      "source": [
        "2. What are the top 10 destination countries by count?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYzco4lWnlyD",
        "outputId": "bad2afb3-f0ba-432e-870a-7b33a4e52857"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+-----------+\n",
            "| DEST_COUNTRY_NAME|total_count|\n",
            "+------------------+-----------+\n",
            "|     United States|   411352.0|\n",
            "|            Canada|     8399.0|\n",
            "|            Mexico|     7140.0|\n",
            "|    United Kingdom|     2025.0|\n",
            "|             Japan|     1548.0|\n",
            "|           Germany|     1468.0|\n",
            "|Dominican Republic|     1353.0|\n",
            "|       South Korea|     1048.0|\n",
            "|       The Bahamas|      955.0|\n",
            "|            France|      935.0|\n",
            "+------------------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"SELECT DEST_COUNTRY_NAME, SUM(count) as total_count\\\n",
        "          FROM summary\\\n",
        "          GROUP BY DEST_COUNTRY_NAME\\\n",
        "          ORDER BY total_count DESC\\\n",
        "          LIMIT 10\\\n",
        "          \").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ib8LIrvtowE4"
      },
      "source": [
        "3. How many flights originated from the United States?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tO2uHnIokC5",
        "outputId": "458f0ac2-d0b2-48e9-a4e8-788db76b8f93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+\n",
            "|sum(count)|\n",
            "+----------+\n",
            "|  411966.0|\n",
            "+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"SELECT SUM(count)\\\n",
        "          FROM summary\\\n",
        "          WHERE ORIGIN_COUNTRY_NAME = 'United States'\\\n",
        "          \").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGbucQSWo7sj"
      },
      "source": [
        "4. What are the top 5 origin countries for flights to Japan?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tflM0zt3o4J7",
        "outputId": "48564135-d687-40f5-f772-da0c1543259c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+-----------+\n",
            "|ORIGIN_COUNTRY_NAME|total_count|\n",
            "+-------------------+-----------+\n",
            "|      United States|     1548.0|\n",
            "+-------------------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"SELECT ORIGIN_COUNTRY_NAME, SUM(count) as total_count\\\n",
        "          FROM summary\\\n",
        "          WHERE DEST_COUNTRY_NAME = 'Japan'\\\n",
        "          GROUP BY ORIGIN_COUNTRY_NAME\\\n",
        "          ORDER BY total_count DESC\\\n",
        "          LIMIT 5\\\n",
        "          \").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uazMY9ocpwiE"
      },
      "source": [
        "5. What is the total number of flights to the United States?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lG8wFlgRpHQK",
        "outputId": "48392fba-12f7-47b2-96af-c78f5fc55cc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+\n",
            "|sum(count)|\n",
            "+----------+\n",
            "|  411352.0|\n",
            "+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"SELECT SUM(count)\\\n",
        "          FROM summary\\\n",
        "          WHERE DEST_COUNTRY_NAME = 'United States'\\\n",
        "          \").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18eyX-WSp9jw"
      },
      "source": [
        "6. What is the total number of flights from the United States?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZZWHdspp3_Q",
        "outputId": "1c799fca-ea4e-4086-92c1-52b45844ab50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+\n",
            "|sum(count)|\n",
            "+----------+\n",
            "|  411966.0|\n",
            "+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"SELECT SUM(count)\\\n",
        "          FROM summary\\\n",
        "          WHERE ORIGIN_COUNTRY_NAME = 'United States'\\\n",
        "          \").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mlRwCalqCSL"
      },
      "source": [
        "7. What are the top 10 origin and destination pairs by count?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wm7two0WqBPC",
        "outputId": "75f21f13-27a5-48e6-e0d1-c481c6fd764a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+-----------------+-----------+\n",
            "|ORIGIN_COUNTRY_NAME|DEST_COUNTRY_NAME|total_count|\n",
            "+-------------------+-----------------+-----------+\n",
            "|      United States|    United States|   370002.0|\n",
            "|             Canada|    United States|     8483.0|\n",
            "|      United States|           Canada|     8399.0|\n",
            "|             Mexico|    United States|     7187.0|\n",
            "|      United States|           Mexico|     7140.0|\n",
            "|      United States|   United Kingdom|     2025.0|\n",
            "|     United Kingdom|    United States|     1970.0|\n",
            "|      United States|            Japan|     1548.0|\n",
            "|              Japan|    United States|     1496.0|\n",
            "|      United States|          Germany|     1468.0|\n",
            "+-------------------+-----------------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"SELECT ORIGIN_COUNTRY_NAME, DEST_COUNTRY_NAME, SUM(count) as total_count\\\n",
        "          FROM summary\\\n",
        "          GROUP BY ORIGIN_COUNTRY_NAME, DEST_COUNTRY_NAME\\\n",
        "          ORDER BY total_count DESC\\\n",
        "          LIMIT 10\\\n",
        "\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "og7dxxw9qYE3"
      },
      "source": [
        "8. How many flights originated from each country?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5ozRf53qNm6",
        "outputId": "ffa198e6-dc28-49f3-c6b3-c5a7c3a1ebb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+-----------+\n",
            "|ORIGIN_COUNTRY_NAME|total_count|\n",
            "+-------------------+-----------+\n",
            "|      United States|   411966.0|\n",
            "|             Canada|     8483.0|\n",
            "|             Mexico|     7187.0|\n",
            "|     United Kingdom|     1970.0|\n",
            "|              Japan|     1496.0|\n",
            "| Dominican Republic|     1420.0|\n",
            "|            Germany|     1336.0|\n",
            "|        The Bahamas|      986.0|\n",
            "|             France|      952.0|\n",
            "|              China|      920.0|\n",
            "|           Colombia|      867.0|\n",
            "|        South Korea|      827.0|\n",
            "|            Jamaica|      712.0|\n",
            "|        Netherlands|      660.0|\n",
            "|             Brazil|      619.0|\n",
            "|         Costa Rica|      608.0|\n",
            "|        El Salvador|      508.0|\n",
            "|               Cuba|      478.0|\n",
            "|             Panama|      465.0|\n",
            "|              Spain|      442.0|\n",
            "+-------------------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"SELECT ORIGIN_COUNTRY_NAME, SUM(count) as total_count\\\n",
        "          FROM summary\\\n",
        "          GROUP BY ORIGIN_COUNTRY_NAME\\\n",
        "          ORDER BY total_count DESC\\\n",
        "\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycJftYB_qsfW"
      },
      "source": [
        "9. How many flights went to each country?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQ1wXYNeqhdi",
        "outputId": "0eec29db-44a2-4c91-deb9-22502710a132"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+-----------+\n",
            "| DEST_COUNTRY_NAME|total_count|\n",
            "+------------------+-----------+\n",
            "|     United States|   411352.0|\n",
            "|            Canada|     8399.0|\n",
            "|            Mexico|     7140.0|\n",
            "|    United Kingdom|     2025.0|\n",
            "|             Japan|     1548.0|\n",
            "|           Germany|     1468.0|\n",
            "|Dominican Republic|     1353.0|\n",
            "|       South Korea|     1048.0|\n",
            "|       The Bahamas|      955.0|\n",
            "|            France|      935.0|\n",
            "|          Colombia|      873.0|\n",
            "|            Brazil|      853.0|\n",
            "|       Netherlands|      776.0|\n",
            "|             China|      772.0|\n",
            "|           Jamaica|      666.0|\n",
            "|        Costa Rica|      588.0|\n",
            "|       El Salvador|      561.0|\n",
            "|            Panama|      510.0|\n",
            "|              Cuba|      466.0|\n",
            "|             Spain|      420.0|\n",
            "+------------------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"SELECT DEST_COUNTRY_NAME, SUM(count) as total_count\\\n",
        "          FROM summary\\\n",
        "          GROUP BY DEST_COUNTRY_NAME\\\n",
        "          ORDER BY total_count DESC\\\n",
        "\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lagDxZ6hrJ7k"
      },
      "source": [
        "10. What is the total number of flights between the United States and Canada?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBvyIFD7qyCB",
        "outputId": "4a3c943e-1ebe-4009-e04f-2032785f72d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+\n",
            "|sum(count)|\n",
            "+----------+\n",
            "|   16882.0|\n",
            "+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"SELECT SUM(count)\\\n",
        "  FROM summary\\\n",
        "  WHERE (ORIGIN_COUNTRY_NAME = 'United States' AND DEST_COUNTRY_NAME = 'Canada') OR (ORIGIN_COUNTRY_NAME = 'Canada' AND DEST_COUNTRY_NAME = 'United States')\\\n",
        "\").show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I56S1Q6zrNQ7"
      },
      "source": [
        "11. What are the 5 most common origin countries for flights to the United Kingdom?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNbP55k6rCIR",
        "outputId": "0894eba9-9ec7-4651-9ba9-abd19b17d3e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+-----------+\n",
            "|ORIGIN_COUNTRY_NAME|total_count|\n",
            "+-------------------+-----------+\n",
            "|      United States|     2025.0|\n",
            "+-------------------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"SELECT ORIGIN_COUNTRY_NAME, SUM(count) as total_count\\\n",
        "  FROM summary\\\n",
        "  WHERE DEST_COUNTRY_NAME = 'United Kingdom'\\\n",
        "  GROUP BY ORIGIN_COUNTRY_NAME\\\n",
        "  ORDER BY total_count DESC\\\n",
        "  LIMIT 5\\\n",
        "\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_A4b86iqrlO7"
      },
      "source": [
        "12. What are the top 10 destination countries for flights from China?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeWWZEVBrVtQ",
        "outputId": "f6db0b8a-da7d-417d-d5c6-db6bcafbadef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-----------+\n",
            "|DEST_COUNTRY_NAME|total_count|\n",
            "+-----------------+-----------+\n",
            "|    United States|      920.0|\n",
            "+-----------------+-----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"\\\n",
        "  SELECT DEST_COUNTRY_NAME, SUM(count) as total_count\\\n",
        "  FROM summary\\\n",
        "  WHERE ORIGIN_COUNTRY_NAME = 'China'\\\n",
        "  GROUP BY DEST_COUNTRY_NAME\\\n",
        "  ORDER BY total_count DESC\\\n",
        "  LIMIT 10\\\n",
        "\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtD_a23lr0XT"
      },
      "source": [
        "13. What is the total number of flights between United States and New Zealand?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-l0oqxgrvDt",
        "outputId": "4f7d9683-5105-4740-ded5-07b727e938f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+\n",
            "|sum(count)|\n",
            "+----------+\n",
            "|     185.0|\n",
            "+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"SELECT SUM(count)\\\n",
        "  FROM summary\\\n",
        "  WHERE (ORIGIN_COUNTRY_NAME = 'United States' AND DEST_COUNTRY_NAME = 'New Zealand') OR (ORIGIN_COUNTRY_NAME = 'New Zealand' AND DEST_COUNTRY_NAME = 'United States')\\\n",
        "\").show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_P6-qrMjsP4z"
      },
      "source": [
        "14. What is the total number of flights from India?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcoWUc6kr88i",
        "outputId": "81dd5a8e-a71d-4b74-c608-48130d57fda2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+\n",
            "|sum(count)|\n",
            "+----------+\n",
            "|      62.0|\n",
            "+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"SELECT SUM(count)\\\n",
        "  FROM summary\\\n",
        "  WHERE ORIGIN_COUNTRY_NAME = 'India'\\\n",
        "\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdric_4ssimz"
      },
      "source": [
        "15. What is the rank of the destination country with the most flights?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BdSFeBisVwN",
        "outputId": "20ef4e0a-5204-4c09-c988-b9e0e8192087"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+-----------+----+\n",
            "| DEST_COUNTRY_NAME|total_count|rank|\n",
            "+------------------+-----------+----+\n",
            "|     United States|   411352.0|   1|\n",
            "|            Canada|     8399.0|   2|\n",
            "|            Mexico|     7140.0|   3|\n",
            "|    United Kingdom|     2025.0|   4|\n",
            "|             Japan|     1548.0|   5|\n",
            "|           Germany|     1468.0|   6|\n",
            "|Dominican Republic|     1353.0|   7|\n",
            "|       South Korea|     1048.0|   8|\n",
            "|       The Bahamas|      955.0|   9|\n",
            "|            France|      935.0|  10|\n",
            "|          Colombia|      873.0|  11|\n",
            "|            Brazil|      853.0|  12|\n",
            "|       Netherlands|      776.0|  13|\n",
            "|             China|      772.0|  14|\n",
            "|           Jamaica|      666.0|  15|\n",
            "|        Costa Rica|      588.0|  16|\n",
            "|       El Salvador|      561.0|  17|\n",
            "|            Panama|      510.0|  18|\n",
            "|              Cuba|      466.0|  19|\n",
            "|             Spain|      420.0|  20|\n",
            "+------------------+-----------+----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"SELECT DEST_COUNTRY_NAME, SUM(count) as total_count,\\\n",
        "  RANK() OVER (ORDER BY SUM(count) DESC) as rank\\\n",
        "  FROM summary\\\n",
        "  GROUP BY DEST_COUNTRY_NAME\\\n",
        "\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vTeroUgs8AR"
      },
      "source": [
        "16. What is the rank of the destination country with the most flights from France?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzXxpZ6css0A",
        "outputId": "770cf795-2487-4181-94d9-5f9efaf341fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-----------+----+\n",
            "|DEST_COUNTRY_NAME|total_count|rank|\n",
            "+-----------------+-----------+----+\n",
            "|    United States|      952.0|   1|\n",
            "+-----------------+-----------+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"SELECT DEST_COUNTRY_NAME, SUM(count) as total_count,\\\n",
        "  RANK() OVER (ORDER BY SUM(count) DESC) as rank\\\n",
        "  FROM summary\\\n",
        "  WHERE ORIGIN_COUNTRY_NAME = 'France'\\\n",
        "  GROUP BY DEST_COUNTRY_NAME\\\n",
        "\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1-C_1H3tJ4w"
      },
      "source": [
        "17. What is the cumulative sum of flights to each destination country, ordered by the number of flights?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OayGJDo-tDP8",
        "outputId": "e440a304-e1b7-4a96-d36e-8a69acacf8ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+-----------+--------------+\n",
            "| DEST_COUNTRY_NAME|total_count|cumulative_sum|\n",
            "+------------------+-----------+--------------+\n",
            "|     United States|   411352.0|      411352.0|\n",
            "|            Canada|     8399.0|      419751.0|\n",
            "|            Mexico|     7140.0|      426891.0|\n",
            "|    United Kingdom|     2025.0|      428916.0|\n",
            "|             Japan|     1548.0|      430464.0|\n",
            "|           Germany|     1468.0|      431932.0|\n",
            "|Dominican Republic|     1353.0|      433285.0|\n",
            "|       South Korea|     1048.0|      434333.0|\n",
            "|       The Bahamas|      955.0|      435288.0|\n",
            "|            France|      935.0|      436223.0|\n",
            "|          Colombia|      873.0|      437096.0|\n",
            "|            Brazil|      853.0|      437949.0|\n",
            "|       Netherlands|      776.0|      438725.0|\n",
            "|             China|      772.0|      439497.0|\n",
            "|           Jamaica|      666.0|      440163.0|\n",
            "|        Costa Rica|      588.0|      440751.0|\n",
            "|       El Salvador|      561.0|      441312.0|\n",
            "|            Panama|      510.0|      441822.0|\n",
            "|              Cuba|      466.0|      442288.0|\n",
            "|             Spain|      420.0|      442708.0|\n",
            "+------------------+-----------+--------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"SELECT DEST_COUNTRY_NAME, SUM(count) as total_count,\\\n",
        "  SUM(SUM(count)) OVER (ORDER BY SUM(count) DESC) as cumulative_sum\\\n",
        "  FROM summary\\\n",
        "  GROUP BY DEST_COUNTRY_NAME\\\n",
        "  ORDER BY total_count DESC\\\n",
        "\").show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}